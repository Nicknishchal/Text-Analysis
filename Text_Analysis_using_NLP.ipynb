{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1dcc3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import re \n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer , sent_tokenize, word_tokenize\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c5c3c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the text files containing positive words\n",
    "positiveWordsList = open('positive-words.txt').read()\n",
    "\n",
    "#reading the text files containing negative words\n",
    "nagitiveWordsList = open('negative-words.txt').read()\n",
    "\n",
    "#reading the text files form the folder which contains all the stop words\n",
    "def load_stop_words(folder):\n",
    "    stop_words = set()\n",
    "    \n",
    "    for file in os.listdir(folder):\n",
    "        file_path = os.path.join(folder, file)\n",
    "        \n",
    "        if file.endswith('.txt'):\n",
    "            try:\n",
    "                # UTF-8 first\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    for line in file:\n",
    "                        stop_words.add(line.strip().lower())\n",
    "            except UnicodeDecodeError:\n",
    "                # If UTF-8 fails,'latin-1' encoding\n",
    "                with open(file_path, 'r', encoding='latin-1') as file:\n",
    "                    for line in file:\n",
    "                        stop_words.add(line.strip().lower())\n",
    "                    \n",
    "    return stop_words\n",
    "\n",
    "folder = 'StopWords'\n",
    "\n",
    "stopWordList = load_stop_words(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "746a102c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ai and ml based youtube analytics and content creation tool for optimizing subscriber engagement and content strategy',\n",
       " 'enhancing front end features and functionality for improved user experience and dashboard accuracy in partner hospital application',\n",
       " 'roas dashboard for campaign wise google ads budget tracking using google ads ap',\n",
       " 'efficient processing and analysis of financial data from pdf files addressing formatting inconsistencies and ensuring data integrity for a toyota dealership management firm',\n",
       " 'development of ea robot for automated trading',\n",
       " 'ai and ml based youtube analytics and content creation tool for optimizing subscriber engagement and content strategy',\n",
       " 'enhancing front end features and functionality for improved user experience and dashboard accuracy in partner hospital application',\n",
       " 'roas dashboard for campaign wise google ads budget tracking using google ads ap',\n",
       " 'efficient processing and analysis of financial data from pdf files addressing formatting inconsistencies and ensuring data integrity for a toyota dealership management firm',\n",
       " 'transforming and managing a large scale sql pedigree database to neo4j graph db',\n",
       " 'enhancing model accuracy from 58 to over 90 strategies for improving predictive performance',\n",
       " 'securing sensitive financial data with privacy preserving machine learning for predictive analytics',\n",
       " 'enhancing data collection for research institutions addressing survey fatigue and incorporating verbal communication for richer insights',\n",
       " 'analyzing the impact of positive emotions and pandemic severity on mental health and resilience among entrepreneurs insights and predictive modeling',\n",
       " 'dynamic brand centric dashboard for automotive dealerships pdf to financial insights with flask react architecture and aws cloud hosting',\n",
       " 'cloud based data modeling and analysis platform with drag and drop interface and openai api integration for simulation insights',\n",
       " 'voter profile analysis and search application for targeted campaign engagement using government voter data',\n",
       " 'bert based classification of individuals and organizations into two categories using natural language processing',\n",
       " 'comprehensive analysis of solana and ethereum contributors using github api with comparative study of 1000 random github profiles',\n",
       " 'powerbi rest api fetching dataflow and refresh schedules with semantic models',\n",
       " 'automated job data import and management solution for enhanced efficiency',\n",
       " 'data analytics and optimization solution for enhancing renewable energy efficiency',\n",
       " 'time series analysis and trend forecasting solution for predicting news trends',\n",
       " 'advanced data visualization solutions for monitoring key business metrics with integrated interactive dashboards',\n",
       " 'advanced patient data analysis solution for trend identification and improved healthcare outcome',\n",
       " 'anomaly detection and analysis for enhanced data integrity and user experience on bright datas website',\n",
       " 'building custom tflite models and benchmarking on voxl2 chips',\n",
       " 'sports prediction model for multiple sports leagues',\n",
       " 'efficient coach allocation system for sports coaching organization',\n",
       " 'data studio dashboard with a data pipeline tool synced with podio using custom webhooks and google cloud function 2',\n",
       " 'ai driven backend for audio to text conversion and analytical assessment in pharmaceutical practice',\n",
       " 'cloud based web application for financial data processing and visualization of sp 500 metrics',\n",
       " 'department wise kpi tracking dashboard with technician performance analysis for atoz dependable service',\n",
       " 'steps to convert a node js api to python for aws lambda deployment',\n",
       " 'building an analytics dashboard with a pdf parsing pipeline for data extraction',\n",
       " 'building a real time log file visualization dashboard in kibana',\n",
       " 'analyzing the impact of female ceo appointments on company stock prices',\n",
       " 'ai chatbot using llm langchain llama',\n",
       " 'healthcare ai chatbot using llama llm langchain',\n",
       " 'ai bot audio to audio',\n",
       " 'recommendation engine for insurance sector to expand business in the rural area',\n",
       " 'data from crm via zapier to google sheets dynamic to powerbi',\n",
       " 'data warehouse to google data studio looker dashboard',\n",
       " 'crm monday com via zapier to power bi dashboard',\n",
       " 'monday com to kpi dashboard to manage view and generate insights from the crm data',\n",
       " 'data management for a political saas application',\n",
       " 'google lsa ads google local service ads etl tools and dashboards',\n",
       " 'ad networks marketing campaign data dashboard in looker google data studio',\n",
       " 'analytical solution for a tech firm',\n",
       " 'ai solution for a technology information and internet firm',\n",
       " 'ai and nlp based solutions to automate data discovery for venture capital and private equity principals',\n",
       " 'an etl solution for an internet publishing firm',\n",
       " 'ai based algorithmic trading bot for forex',\n",
       " 'equity waterfalls model based saas application for real estate sector',\n",
       " 'ai solutions for foreign exchange an automated algo trading tool',\n",
       " 'ai agent development and deployment in jina ai',\n",
       " 'golden record a knowledge graph database approach to unfold discovery using neo4j',\n",
       " 'advanced ai for trading automation',\n",
       " 'create a knowledge graph to provide real time analytics recommendations and a single source of truth',\n",
       " 'advanced ai for thermal person detection',\n",
       " 'advanced ai for road cam threat detection',\n",
       " 'advanced ai for pedestrian crossing safety',\n",
       " 'handgun detection using yolo',\n",
       " 'using graph technology to create single customer view',\n",
       " 'car detection in satellite images',\n",
       " 'building a physics informed neural network for circuit evaluation',\n",
       " 'connecting mongodb database to power bi dashboard dashboard automation',\n",
       " 'data transformation',\n",
       " 'e commerce store analysis purchase behavior ad spend conversion traffic etc',\n",
       " 'kpi dashboard for accountants',\n",
       " 'return on advertising spend dashboard marketing automation and analytics using etl and dashboard',\n",
       " 'ranking customer behaviours for business strategy',\n",
       " 'algorithmic trading for multiple commodities markets like forex metals energy etc',\n",
       " 'trading bot for forex',\n",
       " 'python model for the analysis of sector specific stock etfs for investment purposes%ef%bf%bc',\n",
       " 'medical classification',\n",
       " 'design develop bert question answering model explanations with visualization',\n",
       " 'design and develop solution to anomaly detection classification problems',\n",
       " 'an etl solution for currency data to google big query',\n",
       " 'etl and mlops infrastructure for blockchain analytics',\n",
       " 'an agent based model of a virtual power plant vpp',\n",
       " 'transform api into sdk library and widget',\n",
       " 'integration of a product to a cloud based crm platform',\n",
       " 'a web based dashboard for the filtered data retrieval of land records',\n",
       " 'integration of video conferencing data to the existing web app',\n",
       " 'design develop an app in retool which shows the progress of the added video',\n",
       " 'auvik connectwise integration in grafana',\n",
       " 'data integration and big data performance using elk stack',\n",
       " 'web data connector',\n",
       " 'an app for updating the email id of the user and stripe refund tool using retool',\n",
       " 'an ai ml based web application that detects the correctness of text in a given video',\n",
       " 'website tracking and insights using google analytics google tag manager',\n",
       " 'dashboard to track the analytics of the website using google analytics and google tag manager',\n",
       " 'power bi dashboard on operations transactions and marketing embedding the dashboard to web app',\n",
       " 'nft data automation looksrare and etl tool',\n",
       " 'optimize the data scraper program to easily accommodate large files and solve oom errors',\n",
       " 'making a robust way to sync data from airtables to mongodb using python etl solution',\n",
       " 'incident duration prediction infrastructure and real estate',\n",
       " 'statistical data analysis of reinforced concrete',\n",
       " 'database normalization segmentation with google data studio dashboard insights',\n",
       " 'power bi dashboard to drive insights from complex data to generate business insights',\n",
       " 'real time dashboard to monitor infrastructure activity and machines',\n",
       " 'electric vehicles ev load management system to forecast energy demand',\n",
       " 'power bi data driven map dashboard',\n",
       " 'google local service ads lsa leads dashboard',\n",
       " 'aws lex voice and chatbot',\n",
       " 'metabridges api decentraland integration',\n",
       " 'microsoft azure chatbot with luis language understanding',\n",
       " 'impact of news media and press on innovation startups and investments',\n",
       " 'aws quicksight reporting dashboard',\n",
       " 'google data studio dashboard for marketing ads and traction data',\n",
       " 'gangala in e commerce big data etl elt solution and data warehouse',\n",
       " 'big data solution to an online multivendor marketplace ecommerce business',\n",
       " 'creating a custom report and dashboard using the data got from atera api',\n",
       " 'azure data lake and power bi dashboard',\n",
       " 'google data studio pipeline with gcp mysql',\n",
       " 'quickbooks dashboard to find patterns in finance sales and forecasts',\n",
       " 'marketing sales and financial data business dashboard wink report',\n",
       " 'react native apps in the development portfolio',\n",
       " 'a leading firm website seo optimization',\n",
       " 'a leading hospitality firm in the usa website seo optimization',\n",
       " 'a leading firm in the usa website seo optimization',\n",
       " 'a leading musical instrumental website seo optimization',\n",
       " 'a leading firm in the usa seo and website optimization',\n",
       " 'immigration datawarehouse ai based recommendations',\n",
       " 'lipsync automation for celebrities and influencers',\n",
       " 'key audit matters predictive modeling',\n",
       " 'splitting of songs into its vocals and instrumental',\n",
       " 'ai and ml technologies to evaluate learning assessments',\n",
       " 'datawarehouse and recommendations engine for airbnb',\n",
       " 'real estate data warehouse',\n",
       " 'traction dashboards of marketing campaigns and posts',\n",
       " 'google local service ads lsa data warehouse',\n",
       " 'google local service ads missed calls and messages automation tool',\n",
       " 'marketing ads leads call status data tool to bigquery',\n",
       " 'marketing analytics to automate leads call status and reporting',\n",
       " 'callrail analytics leads report alert',\n",
       " 'marketing automation tool to notify lead details to clients over email and phone',\n",
       " 'data etl local service ads leads to bigquery',\n",
       " 'marbles stimulation using python',\n",
       " 'stocktwits data structurization',\n",
       " 'sentimental analysis on shareholder letter of companies',\n",
       " 'population and community survey of america',\n",
       " 'google lsa api data automation and dashboarding',\n",
       " 'healthcare data analysis',\n",
       " 'budget sales kpi dashboard using power bi',\n",
       " 'amazon buy bot an automation ai tool to auto checkouts']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read the file containing the input URLS\n",
    "input = pd.read_excel(\"Input.xlsx\")\n",
    "input\n",
    "\n",
    "#getting the titles of the URLS\n",
    "def get_title(Url):\n",
    "    titles=[]\n",
    "    for i in range(len(Url)):\n",
    "        heading=Url[i]\n",
    "        clean=heading[heading.index(\"m/\")+2:-1].replace('-',' ')\n",
    "        titles.append(clean)\n",
    "    return titles\n",
    "\n",
    "url=input[\"URL\"]\n",
    "URL_title=get_title(url)\n",
    "URL_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1033d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147\n"
     ]
    }
   ],
   "source": [
    "#printing the total numbers of URLS\n",
    "print(len(URL_title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4de05e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive words\n",
    "with open('positive-words.txt','r') as posfile:\n",
    "    positivewords=posfile.read().lower()\n",
    "positiveWordList=positivewords.split('\\n')\n",
    "\n",
    "\n",
    "# negative words\n",
    "with open('negative-words.txt' ,'r') as negfile:\n",
    "    negativeword=negfile.read().lower()\n",
    "negativeWordList=negativeword.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9724a4a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      https://insights.blackcoffer.com/ai-and-ml-bas...\n",
       "1      https://insights.blackcoffer.com/enhancing-fro...\n",
       "2      https://insights.blackcoffer.com/roas-dashboar...\n",
       "3      https://insights.blackcoffer.com/efficient-pro...\n",
       "4      https://insights.blackcoffer.com/development-o...\n",
       "                             ...                        \n",
       "142    https://insights.blackcoffer.com/population-an...\n",
       "143    https://insights.blackcoffer.com/google-lsa-ap...\n",
       "144    https://insights.blackcoffer.com/healthcare-da...\n",
       "145    https://insights.blackcoffer.com/budget-sales-...\n",
       "146    https://insights.blackcoffer.com/amazon-buy-bo...\n",
       "Name: URL, Length: 147, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "URLS = input [\"URL\"]\n",
    "URLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9b11b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy\n",
      "Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application\n",
      "ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP\n",
      "Efficient Processing and Analysis of Financial Data from PDF Files: Addressing Formatting Inconsistencies and Ensuring Data Integrity for a Toyota Dealership Management Firm\n",
      "Development of EA Robot for Automated Trading\n",
      "AI and ML-Based YouTube Analytics and Content Creation Tool for Optimizing Subscriber Engagement and Content Strategy\n",
      "Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application\n",
      "ROAS Dashboard for Campaign-Wise Google Ads Budget Tracking Using Google Ads AP\n",
      "Efficient Processing and Analysis of Financial Data from PDF Files: Addressing Formatting Inconsistencies and Ensuring Data Integrity for a Toyota Dealership Management Firm\n",
      "Transforming and Managing a Large-Scale SQL Pedigree Database to Neo4j Graph DB\n",
      "Enhancing Model Accuracy from 58% to Over 90%: Strategies for Improving Predictive Performance\n",
      "Securing Sensitive Financial Data with Privacy-Preserving Machine Learning for Predictive Analytics\n",
      "Enhancing Data Collection for Research Institutions: Addressing Survey Fatigue and Incorporating Verbal Communication for Richer Insights\n",
      "Analyzing the Impact of Positive Emotions and Pandemic Severity on Mental Health and Resilience Among Entrepreneurs: Insights and Predictive Modeling\n",
      "Dynamic, Brand-Centric Dashboard for Automotive Dealerships: PDF to Financial Insights with Flask-React Architecture and AWS Cloud Hosting\n",
      "Cloud-Based Data Modeling and Analysis Platform with Drag-and-Drop Interface and OpenAI API Integration for Simulation Insights\n",
      "Voter Profile Analysis and Search Application for Targeted Campaign Engagement Using Government Voter Data\n",
      "BERT-Based Classification of Individuals and Organizations into Two Categories Using Natural Language Processing\n",
      "Comprehensive Analysis of Solana and Ethereum Contributors Using GitHub API with Comparative Study of 1000 Random GitHub Profiles\n",
      "PowerBI REST API – Fetching Dataflow and Refresh Schedules with semantic models\n",
      "Automated Job Data Import and Management Solution for Enhanced Efficiency\n",
      "Data Analytics and Optimization Solution for Enhancing Renewable Energy Efficiency\n",
      "Time Series Analysis and Trend Forecasting Solution for Predicting News Trends\n",
      "Advanced Data Visualization Solutions for Monitoring Key Business Metrics with Integrated, Interactive Dashboards\n",
      "Advanced Patient Data Analysis Solution for Trend Identification and Improved Healthcare Outcome\n",
      "Anomaly Detection and Analysis for Enhanced Data Integrity and User Experience on Bright Data’s Website\n",
      "Building Custom TFLite Models and Benchmarking on VOXL2 Chips\n",
      "Sports Prediction Model for Multiple Sports Leagues\n",
      "Efficient Coach Allocation System for Sports Coaching Organization\n",
      "Data Studio Dashboard with a data pipeline tool synced with Podio using custom Webhooks and Google Cloud Function\n",
      "AI-Driven Backend for Audio-to-Text Conversion and Analytical Assessment in Pharmaceutical Practice\n",
      "Cloud-Based Web Application for Financial Data Processing and Visualization of S&P 500 Metrics\n",
      "Department-Wise KPI Tracking Dashboard with Technician Performance Analysis for AtoZ Dependable Service\n",
      "Steps to Convert a Node.js API to Python for AWS Lambda Deployment\n",
      "Building an Analytics Dashboard with a PDF Parsing Pipeline for Data Extraction\n",
      "Building a Real-Time Log File Visualization Dashboard in Kibana\n",
      "Analyzing the Impact of Female CEO Appointments on Company Stock Prices\n",
      "AI Chatbot using LLM, Langchain, LLama\n",
      "Healthcare AI ChatBot using LLAMA, LLM, Langchain\n",
      "AI Bot Audio to audio\n",
      "Recommendation Engine for Insurance Sector to Expand Business in the Rural Area\n",
      "Data from CRM via Zapier to Google Sheets (Dynamic) to PowerBI\n",
      "Data Warehouse to Google Data Studio (Looker) Dashboard\n",
      "CRM, Monday.com via Zapier to Power BI Dashboard\n",
      "Monday.com to KPI Dashboard to manage, view, and generate insights from the CRM data\n",
      "Data Management for a Political SaaS Application\n",
      "Google LSA Ads (Google Local Service Ads) – ETL tools and Dashboards\n",
      "Ad Networks Marketing Campaign Data Dashboard in Looker (Google Data Studio)\n",
      "Analytical solution for a tech firm\n",
      "AI solution for a Technology, Information and Internet firm\n",
      "AI and NLP-based Solutions to Automate Data Discovery for Venture Capital and Private Equity Principals\n",
      "An ETL solution for an Internet Publishing firm\n",
      "AI-Based Algorithmic Trading Bot for Forex\n",
      "Equity Waterfalls Model-Based SaaS Application for Real Estate Sector\n",
      "AI Solutions for Foreign Exchange – An Automated Algo Trading Tool\n",
      "AI agent development and Deployment in Jina AI\n",
      "Golden Record – A knowledge graph database approach to unfold discovery using Neo4j\n",
      "Advanced AI for Trading Automation\n",
      "Create a Knowledge Graph to Provide Real-time Analytics, Recommendations, and a Single Source of Truth\n",
      "Advanced AI for Thermal Person Detection\n",
      "Advanced AI for Road Cam Threat Detection\n",
      "Advanced AI for Pedestrian Crossing Safety\n",
      "Advanced AI for Handgun Detection\n",
      "Using Graph Technology to Create Single Customer View.\n",
      "Car Detection in Satellite Images\n",
      "Building a Physics-Informed Neural Network for Circuit Evaluation\n",
      "Connecting MongoDB Database to Power BI Dashboard: Dashboard Automation\n",
      "Data Transformation\n",
      "E-commerce Store Analysis – Purchase Behavior, Ad Spend, Conversion, Traffic, etc…\n",
      "KPI Dashboard for Accountants\n",
      "Return on Advertising Spend Dashboard: Marketing Automation and Analytics using ETL and Dashboard\n",
      "Ranking customer behaviours for business strategy\n",
      "Algorithmic trading for multiple commodities markets, like Forex, Metals, Energy, etc.\n",
      "Trading Bot for FOREX\n",
      "Python model for the analysis of sector-specific stock ETFs for investment purposes\n",
      "Medical Classification\n",
      "Design & Develop BERT Question Answering model explanations with visualization\n",
      "Design and develop solution to anomaly detection classification problems\n",
      "An ETL Solution for Currency Data to Google Big Query\n",
      "ETL and MLOps Infrastructure for Blockchain Analytics\n",
      "An agent-based model of a Virtual Power Plant (VPP)\n",
      "Transform API into SDK library and widget\n",
      "Integration of a product to a cloud-based CRM platform\n",
      "A web-based dashboard for the filtered data retrieval of land records\n",
      "Integration of video-conferencing data to the existing web app\n",
      "Design & develop an app in retool which shows the progress of the added video\n",
      "Auvik, Connectwise integration in Grafana\n",
      "Data integration and big data performance using Elasticsearch\n",
      "Web Data Connector\n",
      "An app for updating the email id of the user and stripe refund tool using retool\n",
      "An AI ML-based web application that detects the correctness of text in a given video\n",
      "Website Tracking and Insights using Google Analytics, & Google Tag Manager\n",
      "Dashboard to track the analytics of the website using Google Analytics and Google Tag Manager\n",
      "Power BI Dashboard on Operations, Transactions, and Marketing Data, embedding the Dashboard to Web App\n",
      "NFT Data Automation (looksrare), and ETL tool\n",
      "Optimize the data scraper program to easily accommodate large files and solve OOM errors\n",
      "Making a robust way to sync data from airtables to mongoDB using python – ETL Solution\n",
      "Incident Duration Prediction – Infrastructure and Real Estate\n",
      "Statistical Data Analysis of Reinforced Concrete\n",
      "Database Normalization & Segmentation with Google Data Studio Dashboard Insights\n",
      "Power BI dashboard to drive insights from complex data to generate business insights\n",
      "Real-time dashboard to monitor infrastructure activity and Machines\n",
      "Electric Vehicles (EV) Load Management System to Forecast Energy Demand\n",
      "Power BI Data-Driven Map Dashboard\n",
      "Google Local Service Ads (LSA) Leads Dashboard\n",
      "AWS Lex Voice and Chatbot\n",
      "MetaBridges API Decentraland Integration – AR, VR\n",
      "Microsoft Azure chatbot with LUIS (Language Understanding)\n",
      "Impact of news, media, and press on innovation, startups, and investments\n",
      "AWS QuickSight Reporting Dashboard\n",
      "Google Data Studio Dashboard for Marketing, ads and Traction data\n",
      "Gangala.in: E-commerce Big Data ETL / ELT Solution and Data Warehouse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Big Data solution to an online multivendor marketplace eCommerce business\n",
      "Creating a custom report and dashboard using the data got from Atera API\n",
      "Azure Data Lake and Power BI Dashboard\n",
      "Google Data Studio Pipeline with GCP/MySQL\n",
      "QuickBooks dashboard to find patterns in finance, sales, and forecasts\n",
      "Marketing, sales, and financial data business dashboard (Wink Report)\n",
      "React Native Apps in the Development Portfolio\n",
      "A Leading Law Firm in the USA, Website SEO & Optimization\n",
      "A Leading Hospitality Firm in the USA, Website SEO & Optimization\n",
      "A Leading Firm in the USA, Website SEO & Optimization\n",
      "A Leading Musical Instrumental, Website SEO & Optimization\n",
      "A Leading Firm in the USA, SEO and Website Optimization\n",
      "Immigration Datawarehouse & AI-based recommendations\n",
      "Lipsync Automation for Celebrities and Influencers\n",
      "Key Audit Matters Predictive Modeling\n",
      "Splitting of Songs into its Vocals and Instrumental\n",
      "AI and ML technologies to Evaluate Learning Assessments\n",
      "Datawarehouse, and Recommendations Engine for AirBNB\n",
      "Real Estate Data Warehouse\n",
      "Traction Dashboards of Marketing Campaigns and Posts\n",
      "Google Local Service Ads (LSA) Data Warehouse\n",
      "Google Local Service Ads Missed Calls and Messages Automation Tool\n",
      "Marketing Ads Leads Call Status Data Tool to BigQuery\n",
      "Marketing Analytics to Automate Leads Call Status and Reporting\n",
      "CallRail, Analytics & Leads Report Alert\n",
      "Marketing Tool to Notify Leads to Clients over Email and Phone\n",
      "Data ETL: Local Service Ads Leads to BigQuery\n",
      "Marbles Stimulation using python\n",
      "Stocktwits Data Structurization\n",
      "Sentimental Analysis on Shareholder Letter of Companies\n",
      "Population and Community Survey of America\n",
      "Google LSA API Data Automation and Dashboarding\n",
      "Healthcare Data Analysis\n",
      "Budget, Sales KPI Dashboard using Power BI\n",
      "Amazon Buy Bot, an Automation AI tool to Auto-Checkouts\n"
     ]
    }
   ],
   "source": [
    "data =[]\n",
    "for url in URLS:\n",
    "\n",
    "  page=requests.get(url)  \n",
    "  soup = BeautifulSoup(page.text , 'html.parser')\n",
    "  #get title\n",
    "  title = soup.find(\"h1\",class_=\"entry-title\").get_text()\n",
    "  print(title)\n",
    "  #get the text corrosponding the title \n",
    "  text = soup.find(class_=\"td-post-content tagdiv-type\").get_text()\n",
    "  # print(text)\n",
    "\n",
    "  # removing the space from the front and back\n",
    "  lines=(line.strip() for line in text.splitlines())\n",
    "\n",
    "  # converting into one huge chunk\n",
    "  chunks=(phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "\n",
    "  # drop blank lines\n",
    "  text1= '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "  data.append(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35ba7c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147\n"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b36d98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = [text.replace('\\xa0', ' ') for text in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "639743bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data frame and all the columns of the text\n",
    "df = pd.DataFrame({'title':URL_title,'descp': cleaned_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3f9fe13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizeing module and filtering tokens using stop words list, removing punctuations\n",
    "def tokenizer(text):\n",
    "    text = text.lower()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    filtered_words = list(filter(lambda token: token not in stopWordList, tokens))\n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c5a6cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the positive score of the text\n",
    "def positive_score (text):\n",
    "    posword=0\n",
    "    tokenphrase = tokenizer(text)\n",
    "    for word in tokenphrase :\n",
    "        if word in positiveWordList:\n",
    "            posword+=1 \n",
    "    return posword \n",
    "df[\"positive_score\"] = df[\"descp\"].apply (positive_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "571531b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the negative score of the text\n",
    "def negative_score (text):\n",
    "    negword=0\n",
    "    tokenphrase = tokenizer(text)\n",
    "    for word in tokenphrase :\n",
    "        if word in negativeWordList :\n",
    "            negword +=1\n",
    "    return negword\n",
    "df[\"negative_score\"] = df[\"descp\"].apply (negative_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a396845e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the polarity score of the text\n",
    "def polarity_score (positive_score , negative_score) :\n",
    "  return (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n",
    "df[\"polarity_score\"] = np.vectorize(polarity_score)(df['positive_score'],df['negative_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62676f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the total word count of the text\n",
    "def total_word_count(text):\n",
    "    tokens = tokenizer(text)\n",
    "    return len(tokens)\n",
    "df[\"total word count\"] = df[\"descp\"].apply (total_word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1633221b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # calculating the number of complex words of the text\n",
    "def complex_word_count(text):\n",
    "    tokens = tokenizer(text)\n",
    "    complexWord = 0\n",
    "    \n",
    "    for word in tokens:\n",
    "        vowels=0\n",
    "        if word.endswith(('es','ed')):\n",
    "            pass\n",
    "        else:\n",
    "            for w in word:\n",
    "                if(w=='a' or w=='e' or w=='i' or w=='o' or w=='u'):\n",
    "                    vowels += 1\n",
    "            if(vowels > 2):\n",
    "                complexWord += 1\n",
    "    return complexWord\n",
    "df[\"complex_word_count\"] = df[\"descp\"].apply (complex_word_count)\n",
    "\n",
    "# calculating the percentage of complex words of the text\n",
    "def percentage_complex_word(text):\n",
    "    tokens = tokenizer(text)\n",
    "    complexWord = 0\n",
    "    complex_word_percentage = 0\n",
    "    \n",
    "    for word in tokens:\n",
    "        vowels=0\n",
    "        if word.endswith(('es','ed')):\n",
    "            pass\n",
    "        else:\n",
    "            for w in word:\n",
    "                if(w=='a' or w=='e' or w=='i' or w=='o' or w=='u'):\n",
    "                    vowels += 1\n",
    "            \n",
    "            if(vowels > 2):\n",
    "                complexWord += 1\n",
    "    if len(tokens) != 0:\n",
    "        complex_word_percentage = complexWord/len(tokens)\n",
    "    \n",
    "    return complex_word_percentage\n",
    "df[\"percentage_complex_word\"] = df[\"descp\"].apply (percentage_complex_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11990f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the Average_Words_Per_Sentence of the text\n",
    "def Average_Words_Per_Sentence (text,word_count):\n",
    "    SentenceCount = len (sent_tokenize(text))\n",
    "    if SentenceCount > 0 : Average_Words_Per_Sentence = word_count / SentenceCount\n",
    "    avg = Average_Words_Per_Sentence\n",
    "    return round(avg)\n",
    "df[\"Average_Words_Per_Sentence\"] =np.vectorize(Average_Words_Per_Sentence)(df[\"descp\"],df[\"total word count\"])\n",
    "\n",
    "# calculating the Average_Word_Length of the text\n",
    "def Average_Word_Length(text,word_count):\n",
    "    CharCount=len(text)\n",
    "    if CharCount > 0 : Average_Words_Length = CharCount /word_count \n",
    "    avg = Average_Words_Length\n",
    "    return avg\n",
    "df[\"Average_Word_Length\"] =  np.vectorize(Average_Word_Length)(df[\"descp\"],df[\"total word count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f40388e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the fog index of the text\n",
    "def fog_index(averageSentenceLength, percentageComplexWord):\n",
    "    fogIndex = 0.4 * (averageSentenceLength + percentageComplexWord)\n",
    "    return fogIndex\n",
    "df[\"Fog_Index\"] = np.vectorize(fog_index)(df['Average_Words_Per_Sentence'],df['percentage_complex_word'])\n",
    "\n",
    "# calculating the Subjectivity Score of the text\n",
    "def Subjectivity_Score (positive_score , negative_score , word_count) :\n",
    "  return (positive_score + negative_score) / ((word_count) + 0.000001)\n",
    "df[\"Subjectivity_Score\"] = np.vectorize(Subjectivity_Score)(df['positive_score'],df['negative_score'],df[\"total word count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a7edd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the Syllable_Per_Word of the text\n",
    "def Syllable_Per_Word(text,word_count):\n",
    "    tokens = tokenizer(text)\n",
    "    syllable=0\n",
    "    words='aeiou'\n",
    "    for word in tokens:\n",
    "        vowels=0\n",
    "        if word.endswith(('es','ed')):\n",
    "            pass\n",
    "        else:\n",
    "            for w in word:\n",
    "                if w.lower() in words:\n",
    "                    vowels += 1\n",
    "                    syllable += 1\n",
    "    average = syllable / word_count\n",
    "    return average\n",
    "df[\"Syllable_Per_Word\"] = np.vectorize(Syllable_Per_Word)(df[\"descp\"],df[\"total word count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5033e96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\91905\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "def tokenizer(text):\n",
    "    return word_tokenize(text)\n",
    "def Personal_Pronouns(text):\n",
    "    check = [\"i\", \"we\", \"ours\", \"my\", \"us\"]\n",
    "    token = tokenizer(text)\n",
    "    pp = 0\n",
    "    # Loop through the tokens and check for matches with the pronoun list\n",
    "    for word in token:\n",
    "        cleaned_word = word.strip('.,!?()[]{}\":;').lower()\n",
    "        if cleaned_word in check:\n",
    "            pp += 1\n",
    "    return pp\n",
    "df[\"Personal_Pronouns\"] = np.vectorize(Personal_Pronouns)(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "639e73fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>descp</th>\n",
       "      <th>positive_score</th>\n",
       "      <th>negative_score</th>\n",
       "      <th>polarity_score</th>\n",
       "      <th>total word count</th>\n",
       "      <th>complex_word_count</th>\n",
       "      <th>percentage_complex_word</th>\n",
       "      <th>Average_Words_Per_Sentence</th>\n",
       "      <th>Average_Word_Length</th>\n",
       "      <th>Fog_Index</th>\n",
       "      <th>Subjectivity_Score</th>\n",
       "      <th>Syllable_Per_Word</th>\n",
       "      <th>Personal_Pronouns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ai and ml based youtube analytics and content ...</td>\n",
       "      <td>Client BackgroundClient: A leading IT &amp; tech f...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>174</td>\n",
       "      <td>76</td>\n",
       "      <td>0.436782</td>\n",
       "      <td>35</td>\n",
       "      <td>10.568966</td>\n",
       "      <td>14.174713</td>\n",
       "      <td>0.017241</td>\n",
       "      <td>2.327586</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>enhancing front end features and functionality...</td>\n",
       "      <td>Client BackgroundClient: A leading hospital ch...</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>394</td>\n",
       "      <td>151</td>\n",
       "      <td>0.383249</td>\n",
       "      <td>28</td>\n",
       "      <td>13.362944</td>\n",
       "      <td>11.353299</td>\n",
       "      <td>0.043147</td>\n",
       "      <td>2.200508</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>roas dashboard for campaign wise google ads bu...</td>\n",
       "      <td>Client BackgroundClient: A leading IT &amp; tech f...</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0.538461</td>\n",
       "      <td>288</td>\n",
       "      <td>117</td>\n",
       "      <td>0.406250</td>\n",
       "      <td>18</td>\n",
       "      <td>9.829861</td>\n",
       "      <td>7.362500</td>\n",
       "      <td>0.045139</td>\n",
       "      <td>2.256944</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>efficient processing and analysis of financial...</td>\n",
       "      <td>Client BackgroundClient: A leading automobile ...</td>\n",
       "      <td>21</td>\n",
       "      <td>10</td>\n",
       "      <td>0.354839</td>\n",
       "      <td>397</td>\n",
       "      <td>198</td>\n",
       "      <td>0.498741</td>\n",
       "      <td>50</td>\n",
       "      <td>11.017632</td>\n",
       "      <td>20.199496</td>\n",
       "      <td>0.078086</td>\n",
       "      <td>2.390428</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>development of ea robot for automated trading</td>\n",
       "      <td>Objective:The goal of this project is to build...</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>467</td>\n",
       "      <td>163</td>\n",
       "      <td>0.349036</td>\n",
       "      <td>13</td>\n",
       "      <td>10.113490</td>\n",
       "      <td>5.339615</td>\n",
       "      <td>0.025696</td>\n",
       "      <td>2.239829</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>population and community survey of america</td>\n",
       "      <td>Client BackgroundClient: A leading marketing f...</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>556</td>\n",
       "      <td>182</td>\n",
       "      <td>0.327338</td>\n",
       "      <td>46</td>\n",
       "      <td>10.534173</td>\n",
       "      <td>18.530935</td>\n",
       "      <td>0.035971</td>\n",
       "      <td>2.048561</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>google lsa api data automation and dashboarding</td>\n",
       "      <td>Client BackgroundClient: A leading marketing f...</td>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>784</td>\n",
       "      <td>319</td>\n",
       "      <td>0.406888</td>\n",
       "      <td>25</td>\n",
       "      <td>11.235969</td>\n",
       "      <td>10.162755</td>\n",
       "      <td>0.045918</td>\n",
       "      <td>2.359694</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>healthcare data analysis</td>\n",
       "      <td>Client BackgroundClient: A leading healthcare ...</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.111111</td>\n",
       "      <td>195</td>\n",
       "      <td>71</td>\n",
       "      <td>0.364103</td>\n",
       "      <td>24</td>\n",
       "      <td>12.620513</td>\n",
       "      <td>9.745641</td>\n",
       "      <td>0.092308</td>\n",
       "      <td>2.256410</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>budget sales kpi dashboard using power bi</td>\n",
       "      <td>Project DescriptionWeekly Data – clustered bar...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>71</td>\n",
       "      <td>18</td>\n",
       "      <td>0.253521</td>\n",
       "      <td>71</td>\n",
       "      <td>11.859155</td>\n",
       "      <td>28.501408</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.464789</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>amazon buy bot an automation ai tool to auto c...</td>\n",
       "      <td>Client BackgroundClient: A leading consulting ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>61</td>\n",
       "      <td>25</td>\n",
       "      <td>0.409836</td>\n",
       "      <td>20</td>\n",
       "      <td>12.540984</td>\n",
       "      <td>8.163934</td>\n",
       "      <td>0.032787</td>\n",
       "      <td>2.786885</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>147 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  \\\n",
       "0    ai and ml based youtube analytics and content ...   \n",
       "1    enhancing front end features and functionality...   \n",
       "2    roas dashboard for campaign wise google ads bu...   \n",
       "3    efficient processing and analysis of financial...   \n",
       "4        development of ea robot for automated trading   \n",
       "..                                                 ...   \n",
       "142         population and community survey of america   \n",
       "143    google lsa api data automation and dashboarding   \n",
       "144                           healthcare data analysis   \n",
       "145          budget sales kpi dashboard using power bi   \n",
       "146  amazon buy bot an automation ai tool to auto c...   \n",
       "\n",
       "                                                 descp  positive_score  \\\n",
       "0    Client BackgroundClient: A leading IT & tech f...               3   \n",
       "1    Client BackgroundClient: A leading hospital ch...              10   \n",
       "2    Client BackgroundClient: A leading IT & tech f...              10   \n",
       "3    Client BackgroundClient: A leading automobile ...              21   \n",
       "4    Objective:The goal of this project is to build...               9   \n",
       "..                                                 ...             ...   \n",
       "142  Client BackgroundClient: A leading marketing f...              12   \n",
       "143  Client BackgroundClient: A leading marketing f...              20   \n",
       "144  Client BackgroundClient: A leading healthcare ...               8   \n",
       "145  Project DescriptionWeekly Data – clustered bar...               0   \n",
       "146  Client BackgroundClient: A leading consulting ...               2   \n",
       "\n",
       "     negative_score  polarity_score  total word count  complex_word_count  \\\n",
       "0                 0        1.000000               174                  76   \n",
       "1                 7        0.176471               394                 151   \n",
       "2                 3        0.538461               288                 117   \n",
       "3                10        0.354839               397                 198   \n",
       "4                 3        0.500000               467                 163   \n",
       "..              ...             ...               ...                 ...   \n",
       "142               8        0.200000               556                 182   \n",
       "143              16        0.111111               784                 319   \n",
       "144              10       -0.111111               195                  71   \n",
       "145               0        0.000000                71                  18   \n",
       "146               0        1.000000                61                  25   \n",
       "\n",
       "     percentage_complex_word  Average_Words_Per_Sentence  Average_Word_Length  \\\n",
       "0                   0.436782                          35            10.568966   \n",
       "1                   0.383249                          28            13.362944   \n",
       "2                   0.406250                          18             9.829861   \n",
       "3                   0.498741                          50            11.017632   \n",
       "4                   0.349036                          13            10.113490   \n",
       "..                       ...                         ...                  ...   \n",
       "142                 0.327338                          46            10.534173   \n",
       "143                 0.406888                          25            11.235969   \n",
       "144                 0.364103                          24            12.620513   \n",
       "145                 0.253521                          71            11.859155   \n",
       "146                 0.409836                          20            12.540984   \n",
       "\n",
       "     Fog_Index  Subjectivity_Score  Syllable_Per_Word  Personal_Pronouns  \n",
       "0    14.174713            0.017241           2.327586                  1  \n",
       "1    11.353299            0.043147           2.200508                  7  \n",
       "2     7.362500            0.045139           2.256944                  1  \n",
       "3    20.199496            0.078086           2.390428                  4  \n",
       "4     5.339615            0.025696           2.239829                  1  \n",
       "..         ...                 ...                ...                ...  \n",
       "142  18.530935            0.035971           2.048561                  2  \n",
       "143  10.162755            0.045918           2.359694                  6  \n",
       "144   9.745641            0.092308           2.256410                 13  \n",
       "145  28.501408            0.000000           2.464789                  0  \n",
       "146   8.163934            0.032787           2.786885                  1  \n",
       "\n",
       "[147 rows x 14 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c25b0eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91905\\AppData\\Local\\Temp\\ipykernel_4636\\638239199.py:2: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  final = df.drop(\"descp\" , 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>positive_score</th>\n",
       "      <th>negative_score</th>\n",
       "      <th>polarity_score</th>\n",
       "      <th>total word count</th>\n",
       "      <th>complex_word_count</th>\n",
       "      <th>percentage_complex_word</th>\n",
       "      <th>Average_Words_Per_Sentence</th>\n",
       "      <th>Average_Word_Length</th>\n",
       "      <th>Fog_Index</th>\n",
       "      <th>Subjectivity_Score</th>\n",
       "      <th>Syllable_Per_Word</th>\n",
       "      <th>Personal_Pronouns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ai and ml based youtube analytics and content ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>174</td>\n",
       "      <td>76</td>\n",
       "      <td>0.436782</td>\n",
       "      <td>35</td>\n",
       "      <td>10.568966</td>\n",
       "      <td>14.174713</td>\n",
       "      <td>0.017241</td>\n",
       "      <td>2.327586</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>enhancing front end features and functionality...</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>394</td>\n",
       "      <td>151</td>\n",
       "      <td>0.383249</td>\n",
       "      <td>28</td>\n",
       "      <td>13.362944</td>\n",
       "      <td>11.353299</td>\n",
       "      <td>0.043147</td>\n",
       "      <td>2.200508</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>roas dashboard for campaign wise google ads bu...</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0.538461</td>\n",
       "      <td>288</td>\n",
       "      <td>117</td>\n",
       "      <td>0.406250</td>\n",
       "      <td>18</td>\n",
       "      <td>9.829861</td>\n",
       "      <td>7.362500</td>\n",
       "      <td>0.045139</td>\n",
       "      <td>2.256944</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>efficient processing and analysis of financial...</td>\n",
       "      <td>21</td>\n",
       "      <td>10</td>\n",
       "      <td>0.354839</td>\n",
       "      <td>397</td>\n",
       "      <td>198</td>\n",
       "      <td>0.498741</td>\n",
       "      <td>50</td>\n",
       "      <td>11.017632</td>\n",
       "      <td>20.199496</td>\n",
       "      <td>0.078086</td>\n",
       "      <td>2.390428</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>development of ea robot for automated trading</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>467</td>\n",
       "      <td>163</td>\n",
       "      <td>0.349036</td>\n",
       "      <td>13</td>\n",
       "      <td>10.113490</td>\n",
       "      <td>5.339615</td>\n",
       "      <td>0.025696</td>\n",
       "      <td>2.239829</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>population and community survey of america</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>556</td>\n",
       "      <td>182</td>\n",
       "      <td>0.327338</td>\n",
       "      <td>46</td>\n",
       "      <td>10.534173</td>\n",
       "      <td>18.530935</td>\n",
       "      <td>0.035971</td>\n",
       "      <td>2.048561</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>google lsa api data automation and dashboarding</td>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>784</td>\n",
       "      <td>319</td>\n",
       "      <td>0.406888</td>\n",
       "      <td>25</td>\n",
       "      <td>11.235969</td>\n",
       "      <td>10.162755</td>\n",
       "      <td>0.045918</td>\n",
       "      <td>2.359694</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>healthcare data analysis</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.111111</td>\n",
       "      <td>195</td>\n",
       "      <td>71</td>\n",
       "      <td>0.364103</td>\n",
       "      <td>24</td>\n",
       "      <td>12.620513</td>\n",
       "      <td>9.745641</td>\n",
       "      <td>0.092308</td>\n",
       "      <td>2.256410</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>budget sales kpi dashboard using power bi</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>71</td>\n",
       "      <td>18</td>\n",
       "      <td>0.253521</td>\n",
       "      <td>71</td>\n",
       "      <td>11.859155</td>\n",
       "      <td>28.501408</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.464789</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>amazon buy bot an automation ai tool to auto c...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>61</td>\n",
       "      <td>25</td>\n",
       "      <td>0.409836</td>\n",
       "      <td>20</td>\n",
       "      <td>12.540984</td>\n",
       "      <td>8.163934</td>\n",
       "      <td>0.032787</td>\n",
       "      <td>2.786885</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>147 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  positive_score  \\\n",
       "0    ai and ml based youtube analytics and content ...               3   \n",
       "1    enhancing front end features and functionality...              10   \n",
       "2    roas dashboard for campaign wise google ads bu...              10   \n",
       "3    efficient processing and analysis of financial...              21   \n",
       "4        development of ea robot for automated trading               9   \n",
       "..                                                 ...             ...   \n",
       "142         population and community survey of america              12   \n",
       "143    google lsa api data automation and dashboarding              20   \n",
       "144                           healthcare data analysis               8   \n",
       "145          budget sales kpi dashboard using power bi               0   \n",
       "146  amazon buy bot an automation ai tool to auto c...               2   \n",
       "\n",
       "     negative_score  polarity_score  total word count  complex_word_count  \\\n",
       "0                 0        1.000000               174                  76   \n",
       "1                 7        0.176471               394                 151   \n",
       "2                 3        0.538461               288                 117   \n",
       "3                10        0.354839               397                 198   \n",
       "4                 3        0.500000               467                 163   \n",
       "..              ...             ...               ...                 ...   \n",
       "142               8        0.200000               556                 182   \n",
       "143              16        0.111111               784                 319   \n",
       "144              10       -0.111111               195                  71   \n",
       "145               0        0.000000                71                  18   \n",
       "146               0        1.000000                61                  25   \n",
       "\n",
       "     percentage_complex_word  Average_Words_Per_Sentence  Average_Word_Length  \\\n",
       "0                   0.436782                          35            10.568966   \n",
       "1                   0.383249                          28            13.362944   \n",
       "2                   0.406250                          18             9.829861   \n",
       "3                   0.498741                          50            11.017632   \n",
       "4                   0.349036                          13            10.113490   \n",
       "..                       ...                         ...                  ...   \n",
       "142                 0.327338                          46            10.534173   \n",
       "143                 0.406888                          25            11.235969   \n",
       "144                 0.364103                          24            12.620513   \n",
       "145                 0.253521                          71            11.859155   \n",
       "146                 0.409836                          20            12.540984   \n",
       "\n",
       "     Fog_Index  Subjectivity_Score  Syllable_Per_Word  Personal_Pronouns  \n",
       "0    14.174713            0.017241           2.327586                  1  \n",
       "1    11.353299            0.043147           2.200508                  7  \n",
       "2     7.362500            0.045139           2.256944                  1  \n",
       "3    20.199496            0.078086           2.390428                  4  \n",
       "4     5.339615            0.025696           2.239829                  1  \n",
       "..         ...                 ...                ...                ...  \n",
       "142  18.530935            0.035971           2.048561                  2  \n",
       "143  10.162755            0.045918           2.359694                  6  \n",
       "144   9.745641            0.092308           2.256410                 13  \n",
       "145  28.501408            0.000000           2.464789                  0  \n",
       "146   8.163934            0.032787           2.786885                  1  \n",
       "\n",
       "[147 rows x 13 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#droping the column which is not required\n",
    "final = df.drop(\"descp\" , 1)\n",
    "final "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b5d3d510",
   "metadata": {},
   "outputs": [],
   "source": [
    "final.to_excel('Output_Data_Structure.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d5c58e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b359333",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
